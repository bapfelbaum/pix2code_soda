{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import binutil\n",
    "except ModuleNotFoundError:\n",
    "    import bin.binutil\n",
    "\n",
    "from dreamcoder.program import *\n",
    "from dreamcoder.domains.relation import *\n",
    "from dreamcoder.domains.relation.relation_primitives import *\n",
    "\n",
    "get_baseline_primitives()\n",
    "get_clevr_primitives()\n",
    "get_clevr_primitives_unconfounded()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read txt file\n",
    "mode = \"test\"\n",
    "domain = \"kandinsky\"\n",
    "seed = 2\n",
    "file = f\"../consoleOutputs/{domain}/eval_json/{seed}/{mode}_programs.out\"\n",
    "\n",
    "\n",
    "with open(file, \"r\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "if mode == \"train\":\n",
    "    data = data.split(\"\\n\")\n",
    "    data = [line for line in data if line != \"\"]\n",
    "    task_programs = {}\n",
    "    task_flag = False\n",
    "    for line in data:\n",
    "        if task_flag:\n",
    "            program = line.split(\"\\t\")[1]\n",
    "            task_programs[task_id] = program\n",
    "            task_flag = False\n",
    "        if \"task\" in line:\n",
    "            task_flag = True\n",
    "            task_id = line\n",
    "    NUMBER_TEST_TASKS = len(task_programs)\n",
    "    print(\"Number of all tasks: \", len(task_programs))\n",
    "else:\n",
    "    data = data.split(\"\\n\")\n",
    "    NUMBER_TEST_TASKS = len(data)\n",
    "    print(\"Number of all tasks: \", len(data))\n",
    "    task_programs = {}\n",
    "    for task in data:\n",
    "        if not \"HIT\" in task:\n",
    "            continue\n",
    "        task = task.split(\"w/\")\n",
    "        name = task[0].replace(\" \", \"\")\n",
    "        name = name.replace(\"HIT\", \"\")\n",
    "        program = task[1].split(\";\")[0][1:]\n",
    "        task_programs[name] = program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(task_programs))\n",
    "task_programs\n",
    "\n",
    "cba = 89.67\n",
    "cba_50 = cba * (len(task_programs) / NUMBER_TEST_TASKS) + (\n",
    "    (1 - len(task_programs) / NUMBER_TEST_TASKS) * 50\n",
    ")\n",
    "print(\"cba 50: \", cba_50)\n",
    "print(\"ration solved tasks: \", len(task_programs), \"/\", NUMBER_TEST_TASKS)\n",
    "print(\"percent solved tasks: \", len(task_programs) / NUMBER_TEST_TASKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "path = f\"../data/curi/confounded/train/query\"\n",
    "\n",
    "json_files = [f.path for f in os.scandir(path) if f.path.endswith(\".json\")]\n",
    "\n",
    "task_results = []\n",
    "\n",
    "# iterate over tasks\n",
    "for task_name in tqdm(task_programs.keys()):\n",
    "    task_file = path + \"/\" + task_name + \".json\"\n",
    "    try:\n",
    "        f = open(task_file)\n",
    "        examples = json.load(f)\n",
    "    except:\n",
    "        print(\"task not found: \", task_name)\n",
    "\n",
    "    parsed_examples = []\n",
    "\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    for example in examples:\n",
    "        input = example[\"input\"]\n",
    "        output = example[\"output\"]\n",
    "\n",
    "        # execute program for input\n",
    "        program = task_programs[task_name]\n",
    "        program = Program.parse(program)\n",
    "        try:\n",
    "            program_output = program.evaluate([])(input)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # categorize prediction\n",
    "        if output:\n",
    "            if program_output:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                false_negatives += 1\n",
    "        else:\n",
    "            if program_output:\n",
    "                false_positives += 1\n",
    "            else:\n",
    "                true_negatives += 1\n",
    "\n",
    "    # collect results\n",
    "    results = {\n",
    "        \"task_name\": task_name,\n",
    "        \"TP\": true_positives,\n",
    "        \"FP\": false_positives,\n",
    "        \"TN\": true_negatives,\n",
    "        \"FN\": false_negatives,\n",
    "    }\n",
    "    task_results.append(results)\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(task_results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[\"Accuracy\"] = (results_df[\"TP\"] + results_df[\"TN\"]) / (\n",
    "    results_df[\"TP\"] + results_df[\"TN\"] + results_df[\"FP\"] + results_df[\"FN\"]\n",
    ")\n",
    "results_df[\"CBA\"] = (\n",
    "    (results_df[\"TP\"] / (results_df[\"TP\"] + results_df[\"FN\"]))\n",
    "    + (results_df[\"TN\"] / (results_df[\"TN\"] + results_df[\"FP\"]))\n",
    ") / 2\n",
    "results_df[\"Precision\"] = results_df[\"TP\"] / (results_df[\"TP\"] + results_df[\"FP\"])\n",
    "# get number of tasks with accuracy not nan\n",
    "len(results_df[~results_df[\"Accuracy\"].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort_values(by=\"CBA\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \", mode)\n",
    "# get mean accuracy\n",
    "print(results_df[\"Accuracy\"].mean())\n",
    "# get mean CBA\n",
    "print(results_df[\"CBA\"].mean())\n",
    "# get precision\n",
    "print(results_df[\"Precision\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"kandinsky \", seed)\n",
    "cba = results_df[\"CBA\"].mean()\n",
    "print(\"Class balanced accuracy (solved): \", results_df[\"CBA\"].mean())\n",
    "cba_50 = cba * (len(task_programs) / NUMBER_TEST_TASKS) + (\n",
    "    (1 - (len(task_programs) / NUMBER_TEST_TASKS)) * 0.50\n",
    ")\n",
    "print(\"Class balanced accuracy (all): \", cba_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[\"CBA\"].mean() * (len(results_df) / 100)\n",
    "len(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe\n",
    "results_df.to_csv(f\"../experimentOutputs/kandinsky/kandinsky_{mode}_image_{seed}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get @all accuracies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "domain = \"clevr\"\n",
    "mode = \"test\"\n",
    "NUMBER_TEST_TASKS = 100\n",
    "\n",
    "results = []\n",
    "for seed in range(3):\n",
    "    results_df = pd.read_csv(\n",
    "        f\"../experimentOutputs/{domain}/kandinsky_image_{seed}.csv\"\n",
    "    )\n",
    "    results.append(results_df)\n",
    "\n",
    "mean_accs = [df[\"CBA\"].mean() for df in results]\n",
    "print(mean_accs)\n",
    "\n",
    "cba_all = [0, 0, 0]\n",
    "for seed in range(3):\n",
    "    cba_all[seed] = mean_accs[seed] * (len(results[seed]) / NUMBER_TEST_TASKS) + 0.5 * (\n",
    "        1 - (len(results[seed]) / NUMBER_TEST_TASKS)\n",
    "    )\n",
    "\n",
    "mean_accs = np.array(cba_all) * 100\n",
    "print(mean_accs)\n",
    "mean_acc = np.mean(mean_accs)\n",
    "std = np.std(mean_accs)\n",
    "print(round(mean_acc, 2), round(std, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get @solved accuracies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "results = []\n",
    "for seed in range(3):\n",
    "    results_df = pd.read_csv(\n",
    "        f\"../experimentOutputs/{domain}/kandinsky_image_{seed}.csv\"\n",
    "    )\n",
    "    results.append(results_df)\n",
    "\n",
    "mean_accs = [df[\"CBA\"].mean() for df in results]\n",
    "print(mean_accs)\n",
    "mean_accs = np.array(mean_accs) * 100\n",
    "mean_acc = np.mean(mean_accs)\n",
    "std = np.std(mean_accs)\n",
    "print(round(mean_acc, 2), round(std, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
